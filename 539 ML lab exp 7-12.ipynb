{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EXP-7 Perceptron for digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.  0.  5. ...  0.  0.  0.]\n",
      " [ 0.  0.  0. ... 10.  0.  0.]\n",
      " [ 0.  0.  0. ... 16.  9.  0.]\n",
      " ...\n",
      " [ 0.  0.  1. ...  6.  0.  0.]\n",
      " [ 0.  0.  2. ... 12.  0.  0.]\n",
      " [ 0.  0. 10. ... 12.  1.  0.]]\n",
      "[0 1 2 ... 8 9 8]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9510294936004452"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "from sklearn.linear_model import Perceptron\n",
    "X,y=load_digits(return_X_y=True)\n",
    "clf=Perceptron(tol=1e-3, random_state=0)\n",
    "clf.fit(X,y)\n",
    "print(X)\n",
    "print(y)\n",
    "Perceptron()\n",
    "clf.score(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EXP 8 Feed-Forward Network for wheat seed dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">epoch=0, lrate=0.500, error=6.350\n",
      ">epoch=1, lrate=0.500, error=5.531\n",
      ">epoch=2, lrate=0.500, error=5.221\n",
      ">epoch=3, lrate=0.500, error=4.951\n",
      ">epoch=4, lrate=0.500, error=4.519\n",
      ">epoch=5, lrate=0.500, error=4.173\n",
      ">epoch=6, lrate=0.500, error=3.835\n",
      ">epoch=7, lrate=0.500, error=3.506\n",
      ">epoch=8, lrate=0.500, error=3.192\n",
      ">epoch=9, lrate=0.500, error=2.898\n",
      ">epoch=10, lrate=0.500, error=2.626\n",
      ">epoch=11, lrate=0.500, error=2.377\n",
      ">epoch=12, lrate=0.500, error=2.153\n",
      ">epoch=13, lrate=0.500, error=1.953\n",
      ">epoch=14, lrate=0.500, error=1.774\n",
      ">epoch=15, lrate=0.500, error=1.614\n",
      ">epoch=16, lrate=0.500, error=1.472\n",
      ">epoch=17, lrate=0.500, error=1.346\n",
      ">epoch=18, lrate=0.500, error=1.233\n",
      ">epoch=19, lrate=0.500, error=1.132\n",
      "[{'weights': [-1.4688375095432327, 1.850887325439514, 1.0858178629550297], 'output': 0.029980305604426185, 'delta': -0.0059546604162323625}, {'weights': [0.37711098142462157, -0.0625909894552989, 0.2765123702642716], 'output': 0.9456229000211323, 'delta': 0.0026279652850863837}]\n",
      "[{'weights': [2.515394649397849, -0.3391927502445985, -0.9671565426390275], 'output': 0.23648794202357587, 'delta': -0.04270059278364587}, {'weights': [-2.5584149848484263, 1.0036422106209202, 0.42383086467582715], 'output': 0.7790535202438367, 'delta': 0.03803132596437354}]\n"
     ]
    }
   ],
   "source": [
    "from math import exp\n",
    "from random import seed\n",
    "from random import random\n",
    " \n",
    "# Initialize a network\n",
    "def initialize_network(n_inputs, n_hidden, n_outputs):\n",
    "    network = list()\n",
    "    hidden_layer = [{'weights':[random() for i in range(n_inputs + 1)]} for i in range(n_hidden)]\n",
    "    network.append(hidden_layer)\n",
    "    output_layer = [{'weights':[random() for i in range(n_hidden + 1)]} for i in range(n_outputs)]\n",
    "    network.append(output_layer)\n",
    "    return network\n",
    " \n",
    "# Calculate neuron activation for an input\n",
    "def activate(weights, inputs):\n",
    "\tactivation = weights[-1]\n",
    "\tfor i in range(len(weights)-1):\n",
    "\t\tactivation += weights[i] * inputs[i]\n",
    "\treturn activation\n",
    " \n",
    "# Transfer neuron activation\n",
    "def transfer(activation):\n",
    "\treturn 1.0 / (1.0 + exp(-activation))\n",
    " \n",
    "# Forward propagate input to a network output\n",
    "def forward_propagate(network, row):\n",
    "\tinputs = row\n",
    "\tfor layer in network:\n",
    "\t\tnew_inputs = []\n",
    "\t\tfor neuron in layer:\n",
    "\t\t\tactivation = activate(neuron['weights'], inputs)\n",
    "\t\t\tneuron['output'] = transfer(activation)\n",
    "\t\t\tnew_inputs.append(neuron['output'])\n",
    "\t\tinputs = new_inputs\n",
    "\treturn inputs\n",
    " \n",
    "# Calculate the derivative of an neuron output\n",
    "def transfer_derivative(output):\n",
    "\treturn output * (1.0 - output)\n",
    " \n",
    "# Backpropagate error and store in neurons\n",
    "def backward_propagate_error(network, expected):\n",
    "\tfor i in reversed(range(len(network))):\n",
    "\t\tlayer = network[i]\n",
    "\t\terrors = list()\n",
    "\t\tif i != len(network)-1:\n",
    "\t\t\tfor j in range(len(layer)):\n",
    "\t\t\t\terror = 0.0\n",
    "\t\t\t\tfor neuron in network[i + 1]:\n",
    "\t\t\t\t\terror += (neuron['weights'][j] * neuron['delta'])\n",
    "\t\t\t\terrors.append(error)\n",
    "\t\telse:\n",
    "\t\t\tfor j in range(len(layer)):\n",
    "\t\t\t\tneuron = layer[j]\n",
    "\t\t\t\terrors.append(expected[j] - neuron['output'])\n",
    "\t\tfor j in range(len(layer)):\n",
    "\t\t\tneuron = layer[j]\n",
    "\t\t\tneuron['delta'] = errors[j] * transfer_derivative(neuron['output'])\n",
    " \n",
    "# Update network weights with error\n",
    "def update_weights(network, row, l_rate):\n",
    "\tfor i in range(len(network)):\n",
    "\t\tinputs = row[:-1]\n",
    "\t\tif i != 0:\n",
    "\t\t\tinputs = [neuron['output'] for neuron in network[i - 1]]\n",
    "\t\tfor neuron in network[i]:\n",
    "\t\t\tfor j in range(len(inputs)):\n",
    "\t\t\t\tneuron['weights'][j] += l_rate * neuron['delta'] * inputs[j]\n",
    "\t\t\tneuron['weights'][-1] += l_rate * neuron['delta']\n",
    " \n",
    "# Train a network for a fixed number of epochs\n",
    "def train_network(network, train, l_rate, n_epoch, n_outputs):\n",
    "\tfor epoch in range(n_epoch):\n",
    "\t\tsum_error = 0\n",
    "\t\tfor row in train:\n",
    "\t\t\toutputs = forward_propagate(network, row)\n",
    "\t\t\texpected = [0 for i in range(n_outputs)]\n",
    "\t\t\texpected[row[-1]] = 1\n",
    "\t\t\tsum_error += sum([(expected[i]-outputs[i])**2 for i in range(len(expected))])\n",
    "\t\t\tbackward_propagate_error(network, expected)\n",
    "\t\t\tupdate_weights(network, row, l_rate)\n",
    "\t\tprint('>epoch=%d, lrate=%.3f, error=%.3f' % (epoch, l_rate, sum_error))\n",
    " \n",
    "# Test training backprop algorithm\n",
    "seed(1)\n",
    "dataset = [[2.7810836,2.550537003,0],\n",
    "\t[1.465489372,2.362125076,0],\n",
    "\t[3.396561688,4.400293529,0],\n",
    "\t[1.38807019,1.850220317,0],\n",
    "\t[3.06407232,3.005305973,0],\n",
    "\t[7.627531214,2.759262235,1],\n",
    "\t[5.332441248,2.088626775,1],\n",
    "\t[6.922596716,1.77106367,1],\n",
    "\t[8.675418651,-0.242068655,1],\n",
    "\t[7.673756466,3.508563011,1]]\n",
    "n_inputs = len(dataset[0]) - 1\n",
    "n_outputs = len(set([row[-1] for row in dataset]))\n",
    "network = initialize_network(n_inputs, 2, n_outputs)\n",
    "train_network(network, dataset, 0.5, 20, n_outputs)\n",
    "for layer in network:\n",
    "\tprint(layer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores: [92.85714285714286, 92.85714285714286, 97.61904761904762, 92.85714285714286, 90.47619047619048]\n",
      "Mean Accuracy: 93.333%\n"
     ]
    }
   ],
   "source": [
    "# Backprop on the Seeds Dataset\n",
    "from random import seed\n",
    "from random import randrange\n",
    "from random import random\n",
    "from csv import reader\n",
    "from math import exp\n",
    " \n",
    "# Load a CSV file\n",
    "def load_csv(filename):\n",
    "\tdataset = list()\n",
    "\twith open(filename, 'r') as file:\n",
    "\t\tcsv_reader = reader(file)\n",
    "\t\tfor row in csv_reader:\n",
    "\t\t\tif not row:\n",
    "\t\t\t\tcontinue\n",
    "\t\t\tdataset.append(row)\n",
    "\treturn dataset\n",
    " \n",
    "# Convert string column to float\n",
    "def str_column_to_float(dataset, column):\n",
    "\tfor row in dataset:\n",
    "\t\trow[column] = float(row[column].strip())\n",
    " \n",
    "# Convert string column to integer\n",
    "def str_column_to_int(dataset, column):\n",
    "\tclass_values = [row[column] for row in dataset]\n",
    "\tunique = set(class_values)\n",
    "\tlookup = dict()\n",
    "\tfor i, value in enumerate(unique):\n",
    "\t\tlookup[value] = i\n",
    "\tfor row in dataset:\n",
    "\t\trow[column] = lookup[row[column]]\n",
    "\treturn lookup\n",
    " \n",
    "# Find the min and max values for each column\n",
    "def dataset_minmax(dataset):\n",
    "\tminmax = list()\n",
    "\tstats = [[min(column), max(column)] for column in zip(*dataset)]\n",
    "\treturn stats\n",
    " \n",
    "# Rescale dataset columns to the range 0-1\n",
    "def normalize_dataset(dataset, minmax):\n",
    "\tfor row in dataset:\n",
    "\t\tfor i in range(len(row)-1):\n",
    "\t\t\trow[i] = (row[i] - minmax[i][0]) / (minmax[i][1] - minmax[i][0])\n",
    " \n",
    "# Split a dataset into k folds\n",
    "def cross_validation_split(dataset, n_folds):\n",
    "\tdataset_split = list()\n",
    "\tdataset_copy = list(dataset)\n",
    "\tfold_size = int(len(dataset) / n_folds)\n",
    "\tfor i in range(n_folds):\n",
    "\t\tfold = list()\n",
    "\t\twhile len(fold) < fold_size:\n",
    "\t\t\tindex = randrange(len(dataset_copy))\n",
    "\t\t\tfold.append(dataset_copy.pop(index))\n",
    "\t\tdataset_split.append(fold)\n",
    "\treturn dataset_split\n",
    " \n",
    "# Calculate accuracy percentage\n",
    "def accuracy_metric(actual, predicted):\n",
    "\tcorrect = 0\n",
    "\tfor i in range(len(actual)):\n",
    "\t\tif actual[i] == predicted[i]:\n",
    "\t\t\tcorrect += 1\n",
    "\treturn correct / float(len(actual)) * 100.0\n",
    " \n",
    "# Evaluate an algorithm using a cross validation split\n",
    "def evaluate_algorithm(dataset, algorithm, n_folds, *args):\n",
    "\tfolds = cross_validation_split(dataset, n_folds)\n",
    "\tscores = list()\n",
    "\tfor fold in folds:\n",
    "\t\ttrain_set = list(folds)\n",
    "\t\ttrain_set.remove(fold)\n",
    "\t\ttrain_set = sum(train_set, [])\n",
    "\t\ttest_set = list()\n",
    "\t\tfor row in fold:\n",
    "\t\t\trow_copy = list(row)\n",
    "\t\t\ttest_set.append(row_copy)\n",
    "\t\t\trow_copy[-1] = None\n",
    "\t\tpredicted = algorithm(train_set, test_set, *args)\n",
    "\t\tactual = [row[-1] for row in fold]\n",
    "\t\taccuracy = accuracy_metric(actual, predicted)\n",
    "\t\tscores.append(accuracy)\n",
    "\treturn scores\n",
    " \n",
    "# Calculate neuron activation for an input\n",
    "def activate(weights, inputs):\n",
    "\tactivation = weights[-1]\n",
    "\tfor i in range(len(weights)-1):\n",
    "\t\tactivation += weights[i] * inputs[i]\n",
    "\treturn activation\n",
    " \n",
    "# Transfer neuron activation\n",
    "def transfer(activation):\n",
    "\treturn 1.0 / (1.0 + exp(-activation))\n",
    " \n",
    "# Forward propagate input to a network output\n",
    "def forward_propagate(network, row):\n",
    "\tinputs = row\n",
    "\tfor layer in network:\n",
    "\t\tnew_inputs = []\n",
    "\t\tfor neuron in layer:\n",
    "\t\t\tactivation = activate(neuron['weights'], inputs)\n",
    "\t\t\tneuron['output'] = transfer(activation)\n",
    "\t\t\tnew_inputs.append(neuron['output'])\n",
    "\t\tinputs = new_inputs\n",
    "\treturn inputs\n",
    " \n",
    "# Calculate the derivative of an neuron output\n",
    "def transfer_derivative(output):\n",
    "\treturn output * (1.0 - output)\n",
    " \n",
    "# Backpropagate error and store in neurons\n",
    "def backward_propagate_error(network, expected):\n",
    "\tfor i in reversed(range(len(network))):\n",
    "\t\tlayer = network[i]\n",
    "\t\terrors = list()\n",
    "\t\tif i != len(network)-1:\n",
    "\t\t\tfor j in range(len(layer)):\n",
    "\t\t\t\terror = 0.0\n",
    "\t\t\t\tfor neuron in network[i + 1]:\n",
    "\t\t\t\t\terror += (neuron['weights'][j] * neuron['delta'])\n",
    "\t\t\t\terrors.append(error)\n",
    "\t\telse:\n",
    "\t\t\tfor j in range(len(layer)):\n",
    "\t\t\t\tneuron = layer[j]\n",
    "\t\t\t\terrors.append(expected[j] - neuron['output'])\n",
    "\t\tfor j in range(len(layer)):\n",
    "\t\t\tneuron = layer[j]\n",
    "\t\t\tneuron['delta'] = errors[j] * transfer_derivative(neuron['output'])\n",
    " \n",
    "# Update network weights with error\n",
    "def update_weights(network, row, l_rate):\n",
    "\tfor i in range(len(network)):\n",
    "\t\tinputs = row[:-1]\n",
    "\t\tif i != 0:\n",
    "\t\t\tinputs = [neuron['output'] for neuron in network[i - 1]]\n",
    "\t\tfor neuron in network[i]:\n",
    "\t\t\tfor j in range(len(inputs)):\n",
    "\t\t\t\tneuron['weights'][j] += l_rate * neuron['delta'] * inputs[j]\n",
    "\t\t\tneuron['weights'][-1] += l_rate * neuron['delta']\n",
    " \n",
    "# Train a network for a fixed number of epochs\n",
    "def train_network(network, train, l_rate, n_epoch, n_outputs):\n",
    "\tfor epoch in range(n_epoch):\n",
    "\t\tfor row in train:\n",
    "\t\t\toutputs = forward_propagate(network, row)\n",
    "\t\t\texpected = [0 for i in range(n_outputs)]\n",
    "\t\t\texpected[row[-1]] = 1\n",
    "\t\t\tbackward_propagate_error(network, expected)\n",
    "\t\t\tupdate_weights(network, row, l_rate)\n",
    " \n",
    "# Initialize a network\n",
    "def initialize_network(n_inputs, n_hidden, n_outputs):\n",
    "\tnetwork = list()\n",
    "\thidden_layer = [{'weights':[random() for i in range(n_inputs + 1)]} for i in range(n_hidden)]\n",
    "\tnetwork.append(hidden_layer)\n",
    "\toutput_layer = [{'weights':[random() for i in range(n_hidden + 1)]} for i in range(n_outputs)]\n",
    "\tnetwork.append(output_layer)\n",
    "\treturn network\n",
    " \n",
    "# Make a prediction with a network\n",
    "def predict(network, row):\n",
    "\toutputs = forward_propagate(network, row)\n",
    "\treturn outputs.index(max(outputs))\n",
    " \n",
    "# Backpropagation Algorithm With Stochastic Gradient Descent\n",
    "def back_propagation(train, test, l_rate, n_epoch, n_hidden):\n",
    "\tn_inputs = len(train[0]) - 1\n",
    "\tn_outputs = len(set([row[-1] for row in train]))\n",
    "\tnetwork = initialize_network(n_inputs, n_hidden, n_outputs)\n",
    "\ttrain_network(network, train, l_rate, n_epoch, n_outputs)\n",
    "\tpredictions = list()\n",
    "\tfor row in test:\n",
    "\t\tprediction = predict(network, row)\n",
    "\t\tpredictions.append(prediction)\n",
    "\treturn(predictions)\n",
    " \n",
    "# Test Backprop on Seeds dataset\n",
    "seed(1)\n",
    "# load and prepare data\n",
    "filename = 'seeds_dataset.csv'\n",
    "dataset = load_csv(filename)\n",
    "for i in range(len(dataset[0])-1):\n",
    "\tstr_column_to_float(dataset, i)\n",
    "# convert class column to integers\n",
    "str_column_to_int(dataset, len(dataset[0])-1)\n",
    "# normalize input variables\n",
    "minmax = dataset_minmax(dataset)\n",
    "normalize_dataset(dataset, minmax)\n",
    "# evaluate algorithm\n",
    "n_folds = 5\n",
    "l_rate = 0.3\n",
    "n_epoch = 500\n",
    "n_hidden = 5\n",
    "scores = evaluate_algorithm(dataset, back_propagation, n_folds, l_rate, n_epoch, n_hidden)\n",
    "print('Scores: %s' % scores)\n",
    "print('Mean Accuracy: %.3f%%' % (sum(scores)/float(len(scores))))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # EXP-9 \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# enter the following commands in anaconda prompt\n",
    "python -m pip install --upgrade pip\n",
    "\n",
    "pip install keras\n",
    "\n",
    "conda update wrapt\n",
    "\n",
    "pip install tensorflow\n",
    "\n",
    "pip install numpy --upgrade\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
      "11493376/11490434 [==============================] - 39s 3us/step\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from keras.datasets import mnist\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape (60000, 28, 28)\n",
      "y_train shape (60000,)\n",
      "X_test shape (10000, 28, 28)\n",
      "y_test shape (10000,)\n"
     ]
    }
   ],
   "source": [
    "print(\"X_train shape\", X_train.shape)\n",
    "print(\"y_train shape\", y_train.shape)\n",
    "print(\"X_test shape\", X_test.shape)\n",
    "print(\"y_test shape\", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape before one-hot encoding:  (60000,)\n",
      "Shape after one-hot encoding:  (60000, 10)\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 100)               78500     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                1010      \n",
      "=================================================================\n",
      "Total params: 79,510\n",
      "Trainable params: 79,510\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.6426 - accuracy: 0.8303 - val_loss: 0.1969 - val_accuracy: 0.9422\n",
      "Epoch 2/10\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.1814 - accuracy: 0.9491 - val_loss: 0.1457 - val_accuracy: 0.9570\n",
      "Epoch 3/10\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.1294 - accuracy: 0.9630 - val_loss: 0.1241 - val_accuracy: 0.9648\n",
      "Epoch 4/10\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.1029 - accuracy: 0.9707 - val_loss: 0.1052 - val_accuracy: 0.9678\n",
      "Epoch 5/10\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.0861 - accuracy: 0.9760 - val_loss: 0.0998 - val_accuracy: 0.9706\n",
      "Epoch 6/10\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.0706 - accuracy: 0.9801 - val_loss: 0.0887 - val_accuracy: 0.9729\n",
      "Epoch 7/10\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.0606 - accuracy: 0.9828 - val_loss: 0.0850 - val_accuracy: 0.9743\n",
      "Epoch 8/10\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.0535 - accuracy: 0.9847 - val_loss: 0.0844 - val_accuracy: 0.9746\n",
      "Epoch 9/10\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.0459 - accuracy: 0.9874 - val_loss: 0.0803 - val_accuracy: 0.9749\n",
      "Epoch 10/10\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.0411 - accuracy: 0.9890 - val_loss: 0.0793 - val_accuracy: 0.9762\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1e1b808a7b8>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Conv2D, MaxPool2D\n",
    "from keras.utils import np_utils\n",
    "X_train = X_train.reshape(60000, 784)\n",
    "X_test = X_test.reshape(10000, 784)\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "\n",
    "n_classes = 10\n",
    "print(\"Shape before one-hot encoding: \", y_train.shape)\n",
    "Y_train = np_utils.to_categorical(y_train, n_classes)\n",
    "Y_test = np_utils.to_categorical(y_test, n_classes)\n",
    "print(\"Shape after one-hot encoding: \", Y_train.shape)\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(100, input_shape=(784,), activation='relu'))\n",
    "\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer='adam')\n",
    "\n",
    "model.fit(X_train, Y_train, batch_size=128, epochs=10, validation_data=(X_test, Y_test))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape before one-hot encoding:  (60000,)\n",
      "Shape after one-hot encoding:  (60000, 10)\n",
      "Epoch 1/10\n",
      "469/469 [==============================] - 17s 36ms/step - loss: 0.3926 - accuracy: 0.8864 - val_loss: 0.0776 - val_accuracy: 0.9754\n",
      "Epoch 2/10\n",
      "469/469 [==============================] - 16s 34ms/step - loss: 0.0689 - accuracy: 0.9797 - val_loss: 0.0563 - val_accuracy: 0.9814\n",
      "Epoch 3/10\n",
      "469/469 [==============================] - 17s 35ms/step - loss: 0.0346 - accuracy: 0.9902 - val_loss: 0.0518 - val_accuracy: 0.9832\n",
      "Epoch 4/10\n",
      "469/469 [==============================] - 16s 35ms/step - loss: 0.0193 - accuracy: 0.9948 - val_loss: 0.0574 - val_accuracy: 0.9817\n",
      "Epoch 5/10\n",
      "469/469 [==============================] - 16s 34ms/step - loss: 0.0137 - accuracy: 0.9965 - val_loss: 0.0553 - val_accuracy: 0.9819\n",
      "Epoch 6/10\n",
      "469/469 [==============================] - 16s 34ms/step - loss: 0.0084 - accuracy: 0.9979 - val_loss: 0.0585 - val_accuracy: 0.9826\n",
      "Epoch 7/10\n",
      "469/469 [==============================] - 16s 34ms/step - loss: 0.0066 - accuracy: 0.9983 - val_loss: 0.0598 - val_accuracy: 0.9838\n",
      "Epoch 8/10\n",
      "469/469 [==============================] - 16s 34ms/step - loss: 0.0050 - accuracy: 0.9986 - val_loss: 0.0542 - val_accuracy: 0.9843\n",
      "Epoch 9/10\n",
      "469/469 [==============================] - 16s 35ms/step - loss: 0.0036 - accuracy: 0.9991 - val_loss: 0.0765 - val_accuracy: 0.9798\n",
      "Epoch 10/10\n",
      "469/469 [==============================] - 16s 34ms/step - loss: 0.0068 - accuracy: 0.9977 - val_loss: 0.0656 - val_accuracy: 0.9842\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1e1c95af208>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Conv2D, MaxPool2D, Flatten\n",
    "from keras.utils import np_utils\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "X_train = X_train.reshape(X_train.shape[0], 28, 28, 1)\n",
    "X_test = X_test.reshape(X_test.shape[0], 28, 28, 1)\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "\n",
    "\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "\n",
    "\n",
    "n_classes = 10\n",
    "print(\"Shape before one-hot encoding: \", y_train.shape)\n",
    "Y_train = np_utils.to_categorical(y_train, n_classes)\n",
    "Y_test = np_utils.to_categorical(y_test, n_classes)\n",
    "print(\"Shape after one-hot encoding: \", Y_train.shape)\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(25, kernel_size=(3,3), strides=(1,1), padding='valid', activation='relu', input_shape=(28,28,1)))\n",
    "model.add(MaxPool2D(pool_size=(1,1)))\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(100, activation='relu'))\n",
    "\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer='adam')\n",
    "\n",
    "\n",
    "model.fit(X_train, Y_train, batch_size=128, epochs=10, validation_data=(X_test, Y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> 98.558\n",
      "> 98.767\n",
      "> 98.633\n",
      "> 98.742\n",
      "> 98.750\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\matplotlib\\cbook\\deprecation.py:107: MatplotlibDeprecationWarning: Adding an axes using the same arguments as a previous axes currently reuses the earlier instance.  In a future version, a new instance will always be created and returned.  Meanwhile, this warning can be suppressed, and the future behavior ensured, by passing a unique label to each axes instance.\n",
      "  warnings.warn(message, mplDeprecation, stacklevel=1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: mean=98.690 std=0.081, n=5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from numpy import mean\n",
    "from numpy import std\n",
    "from matplotlib import pyplot\n",
    "from sklearn.model_selection import KFold\n",
    "from keras.datasets import mnist\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.optimizers import SGD\n",
    " \n",
    "def load_dataset():\n",
    "\t\n",
    "\t(trainX, trainY), (testX, testY) = mnist.load_data()\n",
    "\t\n",
    "\ttrainX = trainX.reshape((trainX.shape[0], 28, 28, 1))\n",
    "\ttestX = testX.reshape((testX.shape[0], 28, 28, 1))\n",
    "\t\n",
    "\ttrainY = to_categorical(trainY)\n",
    "\ttestY = to_categorical(testY)\n",
    "\treturn trainX, trainY, testX, testY\n",
    " \n",
    "\n",
    "def prep_pixels(train, test):\n",
    "\t\n",
    "\ttrain_norm = train.astype('float32')\n",
    "\ttest_norm = test.astype('float32')\n",
    "\t\n",
    "\ttrain_norm = train_norm / 255.0\n",
    "\ttest_norm = test_norm / 255.0\n",
    "\t\n",
    "\treturn train_norm, test_norm\n",
    " \n",
    "\n",
    "def define_model():\n",
    "\tmodel = Sequential()\n",
    "\tmodel.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', input_shape=(28, 28, 1)))\n",
    "\tmodel.add(MaxPooling2D((2, 2)))\n",
    "\tmodel.add(Flatten())\n",
    "\tmodel.add(Dense(100, activation='relu', kernel_initializer='he_uniform'))\n",
    "\tmodel.add(Dense(10, activation='softmax'))\n",
    "\t\n",
    "\topt = SGD(lr=0.01, momentum=0.9)\n",
    "\tmodel.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\treturn model\n",
    " \n",
    "\n",
    "def evaluate_model(dataX, dataY, n_folds=5):\n",
    "\tscores, histories = list(), list()\n",
    "\t\n",
    "\tkfold = KFold(n_folds, shuffle=True, random_state=1)\n",
    "\t\n",
    "\tfor train_ix, test_ix in kfold.split(dataX):\n",
    "\t\t\n",
    "\t\tmodel = define_model()\n",
    "\t\t\n",
    "\t\ttrainX, trainY, testX, testY = dataX[train_ix], dataY[train_ix], dataX[test_ix], dataY[test_ix]\n",
    "\t\t\n",
    "\t\thistory = model.fit(trainX, trainY, epochs=10, batch_size=32, validation_data=(testX, testY), verbose=0)\n",
    "\t\t\n",
    "\t\t_, acc = model.evaluate(testX, testY, verbose=0)\n",
    "\t\tprint('> %.3f' % (acc * 100.0))\n",
    "\t\t\n",
    "\t\tscores.append(acc)\n",
    "\t\thistories.append(history)\n",
    "\treturn scores, histories\n",
    " \n",
    "\n",
    "def summarize_diagnostics(histories):\n",
    "\tfor i in range(len(histories)):\n",
    "\t\t\n",
    "\t\tpyplot.subplot(2, 1, 1)\n",
    "\t\tpyplot.title('Cross Entropy Loss')\n",
    "\t\tpyplot.plot(histories[i].history['loss'], color='blue', label='train')\n",
    "\t\tpyplot.plot(histories[i].history['val_loss'], color='orange', label='test')\n",
    "\t\t\n",
    "\t\tpyplot.subplot(2, 1, 2)\n",
    "\t\tpyplot.title('Classification Accuracy')\n",
    "\t\tpyplot.plot(histories[i].history['accuracy'], color='blue', label='train')\n",
    "\t\tpyplot.plot(histories[i].history['val_accuracy'], color='orange', label='test')\n",
    "\tpyplot.show()\n",
    " \n",
    "\n",
    "def summarize_performance(scores):\n",
    "\t\n",
    "\tprint('Accuracy: mean=%.3f std=%.3f, n=%d' % (mean(scores)*100, std(scores)*100, len(scores)))\n",
    "\t\n",
    "\tpyplot.boxplot(scores)\n",
    "\tpyplot.show()\n",
    " \n",
    "\n",
    "def run_test_harness():\n",
    "\n",
    "\ttrainX, trainY, testX, testY = load_dataset()\n",
    "\t\n",
    "\ttrainX, testX = prep_pixels(trainX, testX)\n",
    "\t\n",
    "\tscores, histories = evaluate_model(trainX, trainY)\n",
    "\t\n",
    "\tsummarize_diagnostics(histories)\n",
    "\t\n",
    "\tsummarize_performance(scores)\n",
    " \n",
    "\n",
    "run_test_harness()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EXP 11 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://blog.keras.io/building-autoencoders-in-keras.html\n",
    "\n",
    "import keras\n",
    "from keras import layers\n",
    "\n",
    "\n",
    "encoding_dim = 32  # 32 floats -> compression of factor 24.5, assuming the input is 784 floats\n",
    "\n",
    "# This is our input image\n",
    "input_img = keras.Input(shape=(784,))\n",
    "# \"encoded\" is the encoded representation of the input\n",
    "encoded = layers.Dense(encoding_dim, activation='relu')(input_img)\n",
    "# \"decoded\" is the lossy reconstruction of the input\n",
    "decoded = layers.Dense(784, activation='sigmoid')(encoded)\n",
    "\n",
    "# This model maps an input to its reconstruction\n",
    "autoencoder = keras.Model(input_img, decoded)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This model maps an input to its encoded representation\n",
    "encoder = keras.Model(input_img, encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is our encoded (32-dimensional) input\n",
    "encoded_input = keras.Input(shape=(encoding_dim,))\n",
    "# Retrieve the last layer of the autoencoder model\n",
    "decoder_layer = autoencoder.layers[-1]\n",
    "# Create the decoder model\n",
    "decoder = keras.Model(encoded_input, decoder_layer(encoded_input))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder.compile(optimizer='adam', loss='binary_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "import numpy as np\n",
    "(x_train, _), (x_test, _) = mnist.load_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784)\n",
      "(10000, 784)\n"
     ]
    }
   ],
   "source": [
    "x_train = x_train.astype('float32') / 255.\n",
    "x_test = x_test.astype('float32') / 255.\n",
    "x_train = x_train.reshape((len(x_train), np.prod(x_train.shape[1:])))\n",
    "x_test = x_test.reshape((len(x_test), np.prod(x_test.shape[1:])))\n",
    "print(x_train.shape)\n",
    "print(x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.3781 - val_loss: 0.1900\n",
      "Epoch 2/50\n",
      "235/235 [==============================] - 1s 3ms/step - loss: 0.1796 - val_loss: 0.1523\n",
      "Epoch 3/50\n",
      "235/235 [==============================] - 1s 3ms/step - loss: 0.1482 - val_loss: 0.1327\n",
      "Epoch 4/50\n",
      "235/235 [==============================] - 1s 3ms/step - loss: 0.1308 - val_loss: 0.1207\n",
      "Epoch 5/50\n",
      "235/235 [==============================] - 1s 3ms/step - loss: 0.1195 - val_loss: 0.1123\n",
      "Epoch 6/50\n",
      "235/235 [==============================] - 1s 3ms/step - loss: 0.1118 - val_loss: 0.1065\n",
      "Epoch 7/50\n",
      "235/235 [==============================] - 1s 3ms/step - loss: 0.1065 - val_loss: 0.1025\n",
      "Epoch 8/50\n",
      "235/235 [==============================] - 1s 3ms/step - loss: 0.1028 - val_loss: 0.0993\n",
      "Epoch 9/50\n",
      "235/235 [==============================] - 1s 3ms/step - loss: 0.1000 - val_loss: 0.0970\n",
      "Epoch 10/50\n",
      "235/235 [==============================] - 1s 3ms/step - loss: 0.0976 - val_loss: 0.0954\n",
      "Epoch 11/50\n",
      "235/235 [==============================] - 1s 3ms/step - loss: 0.0963 - val_loss: 0.0944\n",
      "Epoch 12/50\n",
      "235/235 [==============================] - 1s 3ms/step - loss: 0.0954 - val_loss: 0.0937\n",
      "Epoch 13/50\n",
      "235/235 [==============================] - 1s 3ms/step - loss: 0.0949 - val_loss: 0.0933\n",
      "Epoch 14/50\n",
      "235/235 [==============================] - 1s 3ms/step - loss: 0.0944 - val_loss: 0.0929\n",
      "Epoch 15/50\n",
      "235/235 [==============================] - 1s 3ms/step - loss: 0.0941 - val_loss: 0.0928\n",
      "Epoch 16/50\n",
      "235/235 [==============================] - 1s 3ms/step - loss: 0.0937 - val_loss: 0.0926\n",
      "Epoch 17/50\n",
      "235/235 [==============================] - 1s 3ms/step - loss: 0.0938 - val_loss: 0.0925\n",
      "Epoch 18/50\n",
      "235/235 [==============================] - 1s 3ms/step - loss: 0.0939 - val_loss: 0.0924\n",
      "Epoch 19/50\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0934 - val_loss: 0.0922\n",
      "Epoch 20/50\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0934 - val_loss: 0.0923\n",
      "Epoch 21/50\n",
      "235/235 [==============================] - 1s 3ms/step - loss: 0.0934 - val_loss: 0.0921\n",
      "Epoch 22/50\n",
      "235/235 [==============================] - 1s 3ms/step - loss: 0.0934 - val_loss: 0.0921\n",
      "Epoch 23/50\n",
      "235/235 [==============================] - 1s 3ms/step - loss: 0.0930 - val_loss: 0.0920\n",
      "Epoch 24/50\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0934 - val_loss: 0.0920\n",
      "Epoch 25/50\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0931 - val_loss: 0.0919\n",
      "Epoch 26/50\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0930 - val_loss: 0.0919\n",
      "Epoch 27/50\n",
      "235/235 [==============================] - 1s 3ms/step - loss: 0.0933 - val_loss: 0.0919\n",
      "Epoch 28/50\n",
      "235/235 [==============================] - 1s 3ms/step - loss: 0.0931 - val_loss: 0.0918\n",
      "Epoch 29/50\n",
      "235/235 [==============================] - 1s 3ms/step - loss: 0.0929 - val_loss: 0.0919\n",
      "Epoch 30/50\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0929 - val_loss: 0.0918\n",
      "Epoch 31/50\n",
      "235/235 [==============================] - 1s 3ms/step - loss: 0.0931 - val_loss: 0.0918\n",
      "Epoch 32/50\n",
      "235/235 [==============================] - 1s 3ms/step - loss: 0.0930 - val_loss: 0.0918\n",
      "Epoch 33/50\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0930 - val_loss: 0.0918\n",
      "Epoch 34/50\n",
      "235/235 [==============================] - 1s 3ms/step - loss: 0.0930 - val_loss: 0.0918\n",
      "Epoch 35/50\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0929 - val_loss: 0.0918\n",
      "Epoch 36/50\n",
      "235/235 [==============================] - 1s 3ms/step - loss: 0.0928 - val_loss: 0.0918\n",
      "Epoch 37/50\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0929 - val_loss: 0.0917\n",
      "Epoch 38/50\n",
      "235/235 [==============================] - 1s 3ms/step - loss: 0.0927 - val_loss: 0.0918\n",
      "Epoch 39/50\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0928 - val_loss: 0.0917\n",
      "Epoch 40/50\n",
      "235/235 [==============================] - 1s 3ms/step - loss: 0.0927 - val_loss: 0.0917\n",
      "Epoch 41/50\n",
      "235/235 [==============================] - 1s 3ms/step - loss: 0.0929 - val_loss: 0.0917\n",
      "Epoch 42/50\n",
      "235/235 [==============================] - 1s 3ms/step - loss: 0.0927 - val_loss: 0.0917\n",
      "Epoch 43/50\n",
      "235/235 [==============================] - 1s 3ms/step - loss: 0.0927 - val_loss: 0.0917\n",
      "Epoch 44/50\n",
      "235/235 [==============================] - 1s 3ms/step - loss: 0.0924 - val_loss: 0.0918\n",
      "Epoch 45/50\n",
      "235/235 [==============================] - 1s 3ms/step - loss: 0.0930 - val_loss: 0.0917\n",
      "Epoch 46/50\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0925 - val_loss: 0.0916\n",
      "Epoch 47/50\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0928 - val_loss: 0.0916\n",
      "Epoch 48/50\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0926 - val_loss: 0.0916\n",
      "Epoch 49/50\n",
      "235/235 [==============================] - 1s 3ms/step - loss: 0.0926 - val_loss: 0.0916\n",
      "Epoch 50/50\n",
      "235/235 [==============================] - 1s 3ms/step - loss: 0.0926 - val_loss: 0.0916\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1e1ba701908>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "autoencoder.fit(x_train, x_train,\n",
    "                epochs=50,\n",
    "                batch_size=256,\n",
    "                shuffle=True,\n",
    "                validation_data=(x_test, x_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode and decode some digits\n",
    "# Note that we take them from the *test* set\n",
    "encoded_imgs = encoder.predict(x_test)\n",
    "decoded_imgs = decoder.predict(encoded_imgs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x288 with 20 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Use Matplotlib (don't ask)\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "n = 10  # How many digits we will display\n",
    "plt.figure(figsize=(20, 4))\n",
    "for i in range(n):\n",
    "    # Display original\n",
    "    ax = plt.subplot(2, n, i + 1)\n",
    "    plt.imshow(x_test[i].reshape(28, 28))\n",
    "    plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "\n",
    "    # Display reconstruction\n",
    "    ax = plt.subplot(2, n, i + 1 + n)\n",
    "    plt.imshow(decoded_imgs[i].reshape(28, 28))\n",
    "    plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EXP 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
      "17465344/17464789 [==============================] - 71s 4us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<string>:6: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "C:\\Users\\Admin\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\datasets\\imdb.py:159: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  x_train, y_train = np.array(xs[:idx]), np.array(labels[:idx])\n",
      "C:\\Users\\Admin\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\datasets\\imdb.py:160: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  x_test, y_test = np.array(xs[idx:]), np.array(labels[idx:])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 500, 32)           160000    \n",
      "_________________________________________________________________\n",
      "flatten_6 (Flatten)          (None, 16000)             0         \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 250)               4000250   \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 1)                 251       \n",
      "=================================================================\n",
      "Total params: 4,160,501\n",
      "Trainable params: 4,160,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/2\n",
      "196/196 - 6s - loss: 0.4612 - accuracy: 0.7599 - val_loss: 0.2861 - val_accuracy: 0.8785\n",
      "Epoch 2/2\n",
      "196/196 - 6s - loss: 0.1773 - accuracy: 0.9344 - val_loss: 0.3104 - val_accuracy: 0.8713\n",
      "Accuracy: 87.13%\n"
     ]
    }
   ],
   "source": [
    "# MLP for the IMDB problem\n",
    "from keras.datasets import imdb\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "# load the dataset but only keep the top n words, zero the rest\n",
    "top_words = 5000\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=top_words)\n",
    "max_words = 500\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=max_words)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=max_words)\n",
    "# create the model\n",
    "model = Sequential()\n",
    "model.add(Embedding(top_words, 32, input_length=max_words))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(250, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()\n",
    "# Fit the model\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=2, batch_size=128, verbose=2)\n",
    "# Final evaluation of the model\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
